import streamlit as st
from google import genai
from google.genai import types, errors
import time
import os

# ==== ì„¤ì • ë¶€ë¶„ ====
api_key = os.getenv("GOOGLE_API_KEY")
STORE_DISPLAY_NAME = "presto_docs_store"

if not api_key:
    st.error("í™˜ê²½ ë³€ìˆ˜ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. (GOOGLE_API_KEY)")
    st.stop()

# ==== í´ë¼ì´ì–¸íŠ¸ & ìŠ¤í† ì–´ ====
@st.cache_resource
def get_client():
    return genai.Client(api_key=api_key)

@st.cache_resource
def get_store(display_name: str):
    client = get_client()
    for s in client.file_search_stores.list():
        if getattr(s, "display_name", None) == display_name:
            return s
    # ì—†ìœ¼ë©´ ìƒì„±
    store = client.file_search_stores.create(
        config={"display_name": display_name}
    )
    return store


def ask_question(store_name: str, history: list[types.Content]) -> str:
    """File Searchê°€ ì—°ê²°ëœ Geminiì—ê²Œ ì§ˆë¬¸ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    client = get_client()
    max_retries = 5
    delay = 2  # ì´ˆ

    for attempt in range(1, max_retries + 1):
        try:
            response = client.models.generate_content(
                model="gemini-3-pro-preview", #"gemini-2.5-flash",
                contents=history,
                config=types.GenerateContentConfig(
                    tools=[
                        types.Tool(
                            file_search=types.FileSearch(
                                file_search_store_names=[store_name]
                            )
                        )
                    ]
                ),
            )
            return response.text or ""

        except errors.ServerError as e:
            # 503 ê°™ì€ ì„œë²„ ê³¼ë¶€í•˜ ì—ëŸ¬ë§Œ ì¬ì‹œë„
            if "overloaded" in str(e) and attempt < max_retries:
                print(
                    f"[ServerError] ê³¼ë¶€í•˜, {attempt}íšŒ ì‹œë„ ì‹¤íŒ¨ â†’ {delay}ì´ˆ í›„ ì¬ì‹œë„"
                )
                time.sleep(delay)
                continue

            st.error("í˜„ì¬ Gemini ì„œë²„ê°€ ê³¼ë¶€í•˜ ìƒíƒœì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
            print("[ServerError ìµœì¢… ì‹¤íŒ¨]", e)
            return ""

        except errors.APIError as e:
            st.error(f"Gemini API ì—ëŸ¬ ë°œìƒ: {e}")
            print("[APIError]", e)
            return ""


# ==== Streamlit UI ====
st.set_page_config(page_title="Presto Docs Chat", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ“˜ Presto Docs Chat (File Search Preview)")

store = get_store(STORE_DISPLAY_NAME)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "history" not in st.session_state:
    st.session_state.history = []  # type: list[types.Content]

if "last_request_time" not in st.session_state:
    st.session_state.last_request_time = 0.0

# --- ì‚¬ì´ë“œë°”: ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ (ChatGPT ì™¼ìª½ ë¦¬ìŠ¤íŠ¸ ëŠë‚Œ) ---
st.sidebar.header("ì§ˆë¬¸ íˆìŠ¤í† ë¦¬")
user_messages = [m.parts[0].text for m in st.session_state.history if m.role == "user"]

if user_messages:
    for i, q in enumerate(user_messages[-20:], 1):
        # ë§ˆì§€ë§‰ 20ê°œë§Œ í‘œì‹œ
        st.sidebar.markdown(f"**{i}.** {q}")
else:
    st.sidebar.caption("ì•„ì§ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")

# --- ë©”ì¸ ì˜ì—­: ê¸°ì¡´ ëŒ€í™” í‘œì‹œ (ChatGPT ëŒ€í™”ì°½ ëŠë‚Œ) ---
for msg in st.session_state.history:
    if msg.role == "user":
        with st.chat_message("user"):
            st.markdown(msg.parts[0].text)
    else:
        with st.chat_message("assistant"):
            st.markdown(msg.parts[0].text)

# --- ì…ë ¥ì°½ ---
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ACSPLì—ì„œ Enable ì‚¬ìš©í•˜ëŠ” ë²• ì•Œë ¤ì¤˜)")

MAX_TURNS = 6  # user+assistant ìŒ 6ê°œ ì •ë„ë©´ ì¶©ë¶„

if user_input:
    now = time.time()
    # ìš”ì²­ ê°„ê²© ì œí•œ (ë„ˆë¬´ ë¹ ë¥¸ ì—°íƒ€ ë°©ì§€)
    if now - st.session_state.last_request_time < 1.5:
        st.warning("ìš”ì²­ ê°„ê²©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        st.session_state.last_request_time = now

        # 1) í™”ë©´ì— ìœ ì € ë©”ì‹œì§€ í‘œì‹œ + history ë°˜ì˜
        user_msg = types.Content(
            role="user",
            parts=[types.Part(text=user_input)]
        )
        st.session_state.history.append(user_msg)

        with st.chat_message("user"):
            st.markdown(user_input)

        # 2) ëª¨ë¸ í˜¸ì¶œ
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                answer = ask_question(store.name, st.session_state.history)
                if answer:
                    st.markdown(answer)
                else:
                    st.markdown("âš ï¸ ì‘ë‹µì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì„œë²„ ê³¼ë¶€í•˜ ë˜ëŠ” API ì—ëŸ¬)")

        # 3) ëª¨ë¸ ì‘ë‹µë„ historyì— ì¶”ê°€
        model_msg = types.Content(
            role="model",
            parts=[types.Part(text=answer)]
        )
        st.session_state.history.append(model_msg)

        # íˆìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ë’¤ì—ì„œ Ní„´ë§Œ ë‚¨ê¸°ê¸°
        if len(st.session_state.history) > MAX_TURNS * 2:
            st.session_state.history = st.session_state.history[-MAX_TURNS * 2:]
